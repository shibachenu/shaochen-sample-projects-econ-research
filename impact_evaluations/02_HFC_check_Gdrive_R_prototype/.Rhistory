significant_sorted_coeffs <- sorted_coeffs[sorted_coeffs[, "Pr(>|t|)"] < 0.05, ]
# Convert to a data frame for better display
significant_sorted_coeffs_df <- as.data.frame(significant_sorted_coeffs)
# Format p-values to 3 decimal places
significant_sorted_coeffs_df$`Pr(>|t|)` <- format(significant_sorted_coeffs_df$`Pr(>|t|)`, digits = 3, scientific = FALSE)
# Print the results in a nice format
print(significant_sorted_coeffs_df)
predicions = predict(lm_model, newdata = test)
lm_mse <- mean((predictions - test$secondary_enrollment_rate)^2)
predicions = predict(lm_model, newdata = test)
lm_mse <- mean((predictions - test$secondary_enrollment_rate)^2)
predicions = predict(lm_model, newdata = test)
lm_mse <- mean((predicions - test$secondary_enrollment_rate)^2)
lm_mse <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
predicions = predict(rf_model, newdata = test)
train_clean = na.omit(train)
sample_pct = 0.1
train_clean_sampled <- train_clean %>% sample_frac(sample_pct)
rf_model = randomForest(secondary_enrollment_rate~., data=train_clean_sampled)
library("randomForest") # For Random Forest model
library("randomForest") # For Random Forest model
train_clean = na.omit(train)
sample_pct = 0.1
train_clean_sampled <- train_clean %>% sample_frac(sample_pct)
rf_model = randomForest(secondary_enrollment_rate~., data=train_clean_sampled)
print(rf_model)
predicions = predict(rf_model, test)
rf_mse <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
library("randomForest") # For Random Forest model
train_clean = na.omit(train)
sample_pct = 0.1
train_clean_sampled <- train_clean %>% sample_frac(sample_pct)
rf_model = randomForest(secondary_enrollment_rate~., data=train_clean)
print(rf_model)
predicions = predict(rf_model, test)
rf_mse <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
svm_model = svm(secondary_enrollment_rate~., data=train)
library("e1071")        # For SVM model
svm_model = svm(secondary_enrollment_rate~., data=train)
print(svm_model)
predicions = predict(svm_model, test)
svm_mse <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
library(nnet)
# Identify columns with zero variance
constant_columns <- sapply(train_clean, function(x) var(x) == 0)
# Remove these columns from the dataset
train_clean <- train_clean[, !constant_columns]
x_train = train_clean %>%
select(-secondary_enrollment_rate)
y_train = train_clean %>%
select(secondary_enrollment_rate)
# Now perform PCA
pca_result <- prcomp(x_train, scale. = TRUE)
# Extract the proportion of variance explained by each PC
explained_variance <- pca_result$sdev^2 / sum(pca_result$sdev^2)
# Cumulative variance plot
cumulative_variance <- cumsum(explained_variance)
plot(cumulative_variance, xlab = "Number of Principal Components",
ylab = "Cumulative Proportion of Variance Explained",
type = "b",
main = "Cumulative Variance Plot")
#Pick the first 80 PCs based on PCA plots
N = 80
pca_train = cbind(pca_result$x[, 1:N], y_train)
nn_model <- nnet(secondary_enrollment_rate ~ ., data = pca_train, size = 10)
print(nn_model)
x_test = test %>%
select(-secondary_enrollment_rate)
y_test = test %>%
select(secondary_enrollment_rate)
pca_test = prcomp(x_test,  scale. = TRUE)
constant_columns <- sapply(test, function(x) var(x) == 0)
test_clean <- test[, !constant_columns]
constant_columns <- sapply(test, function(x) var(x) == 0)
constant_columns <- sapply(test, function(x) var(x) == 0)
test_clean <- test[, !constant_columns]
# Identify columns with zero variance
constant_columns <- sapply(test, function(x) var(x) == 0)
test_clean <- test[, !
# Identify columns with zero variance
constant_columns <- sapply(test, function(x) var(x) == 0)
test_clean <- test[, !constant_columns]
test_clean <- test[, !constant_columns]
# Identify columns with zero variance
zero_variance_cols <- sapply(test, function(x) var(x) == 0)
# Remove these columns from the data frame
test_clean <- test[, !zero_variance_cols]
# Check for zero variance columns
zero_variance_cols <- sapply(test, function(x) var(x) == 0)
# Ensure there are columns to remove and then remove them
if (any(zero_variance_cols)) {
test_clean <- test[, !zero_variance_cols]
} else {
test_clean <- test
}
x_test = test %>%
select(-secondary_enrollment_rate)
y_test = test %>%
select(secondary_enrollment_rate)
pca_test = prcomp(x_test,  scale. = TRUE)
test_clean = na.omit(test)
test_clean <- test_clean[, !constant_columns]
constant_columns <- sapply(test_clean, function(x) var(x) == 0)
test_clean <- test_clean[, !constant_columns]
x_test = test_clean %>%
select(-secondary_enrollment_rate)
y_test = test_clean %>%
select(secondary_enrollment_rate)
pca_test = prcomp(x_test,  scale. = TRUE)
pca_test = cbind(pca_test$x[, 1:N], y_test)
predicions = predict(nn_model, pca_test)
mse_nn <- mean((predicions - y_test)^2, na.rm = TRUE)
predicions = predict(nn_model, pca_test)
View(predicions)
predicions = predict(nn_model, x_test)
predicions = predict(nn_model, pca_test)
mse_nn <- mean((predicions - y_test)^2, na.rm = TRUE)
View(y_test)
sum(predicions-y_test)
sum((predicions - y_test)^2)
mean((predicions - y_test)^2)
# Check the type of predictions and y_test
str(predictions)
predicions = predict(nn_model, pca_test)
mse_nn <- mean((predicions - y_test)^2, na.rm = TRUE)
# Check the type of predictions and y_test
str(predictions)
predicions = predict(nn_model, pca_test)
mse_nn <- mean((predicions - y_test)^2, na.rm = TRUE)
# Check the type of predictions and y_test
str(predicions)
str(y_test)
predicions = as.numeric(predict(nn_model, pca_test))
mse_nn <- mean((predicions - y_test)^2, na.rm = TRUE)
predicions = as.numeric(predict(nn_model, pca_test))
y_test = as.numeric(y_test)
View(y_test)
predicions = as.numeric(predict(nn_model, pca_test))
# Assuming y_test is a list
y_test <- unlist(y_test)
# Now convert it to numeric (if needed)
y_test <- as.numeric(y_test)
# Check the result
str(y_test)
mse_nn <- mean((predicions - y_test)^2, na.rm = TRUE)
mse_df = data.frame(
MSE_lm = mse_lm,
MSE_rf = mse_rf,
MSE_svm = mse_svm,
MSE_nn = mse_nn
)
mse_lm = 0
mse_rf = 0
mse_df = data.frame(
MSE_lm = mse_lm,
MSE_rf = mse_rf,
MSE_svm = mse_svm,
MSE_nn = mse_nn
)
mse_svm = 0
mse_df = data.frame(
MSE_lm = mse_lm,
MSE_rf = mse_rf,
MSE_svm = mse_svm,
MSE_nn = mse_nn
)
mse_df = data.frame(
MSE_lm = mse_lm,
MSE_rf = mse_rf,
MSE_svm = mse_svm,
MSE_nn = mse_nn
)
# Load the kableExtra package
library(kableExtra)
mse_df = data.frame(
MSE_lm = mse_lm,
MSE_rf = mse_rf,
MSE_svm = mse_svm,
MSE_nn = mse_nn
)
# Load the kableExtra package
library(kableExtra)
# Display the data frame as a nicely formatted table
kable(mse_df, caption = "Performance (MSE) of different prediction models") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
# Load the kableExtra package
library(kableExtra)
mse_df = data.frame(
MSE_lm = mse_lm,
MSE_rf = mse_rf,
MSE_svm = mse_svm,
MSE_nn = mse_nn
)
colnames(mse_df) = c("Linear Regression", "Random Forest", "Support Vector Machine", "Neural Network")
# Display the data frame as a nicely formatted table
kable(mse_df, caption = "Performance (MSE) of different prediction models") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
lm_model = lm(secondary_enrollment_rate~., data=train)
# Get the summary of the model
summary_model <- summary(lm_model)
# Extract coefficients and their p-values
coefficients <- summary_model$coefficients
# Sort coefficients by p-value in ascending order
sorted_coeffs <- coefficients[order(coefficients[, "Pr(>|t|)"]), ]
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_sorted_coeffs <- sorted_coeffs[sorted_coeffs[, "Pr(>|t|)"] < 0.05, ]
# Convert to a data frame for better display
significant_sorted_coeffs_df <- as.data.frame(significant_sorted_coeffs)
# Format p-values to 3 decimal places
significant_sorted_coeffs_df$`Pr(>|t|)` <- format(significant_sorted_coeffs_df$`Pr(>|t|)`, digits = 3, scientific = FALSE)
# Print the results in a nice format
print(significant_sorted_coeffs_df)
predicions = predict(lm_model, newdata = test)
mse_lm <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
hh_pam_results = pam(hh_sampled,
k = 3,
metric = "euclidean",
stand = FALSE)
#Some optimality analysis for # of clusters: a.k.a. silhouette
#Give the size of the dataset, sample only 20% of these households
sample_pct = 0.1
hh_sampled <- hh_filtered %>% sample_frac(sample_pct)
fviz_nbclust(hh_sampled, pam, method ="silhouette")+theme_minimal()
#pam(hh_constructed, k,metric =“manhattan”,stand =FALSE)
hh_pam_results = pam(hh_sampled,
k = 3,
metric = "euclidean",
stand = FALSE)
fviz_cluster(hh_pam_results,
#palette = hh_pam_results$clustering,
ellipse.type ="euclid",
repel =TRUE,
ggtheme =theme_minimal(),
geom = "point")
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
setwd("C:/Users/wb629244/PSSN/pssn2livelihood/03_Endline/03_Endline_HFCs/02_HFC_check_Gdrive_R_prototype")
# Load necessary libraries
library(tidyverse)
library(haven)
library(lubridate)
library(dplyr)
library(tidyr)
library(ggplot2)
library(cluster)
library(factoextra)
source("global_env_setup.R")
source("HFC_utils.R")
library("caret")       # For training models
# Filter out columns with too many missing values (e.g. >20%)
hh_constructed = read_dta(file.path(Sys.getenv("analysis_baseline_data_dir"), "HHSurvey.dta"))
hh_numeric <- hh_constructed %>% select(where(is.numeric))
threshold = 0.2
hh_pred <- hh_numeric  %>%
select(where(~ mean(is.na(.)) <= threshold))
write.csv(colnames(hh_pred),"hh_pred_cols.csv")
#secondary enrollment rate
hh_pred = hh_pred %>%
filter(nb_children_14andabove > 0) %>%
transform(secondary_enrollment_rate = (nb_child_ordsecondary +nb_child_advsecondary)/ nb_children_14andabove) %>%
filter(!is.na(secondary_enrollment_rate)) %>%
select(-nb_aged_6to13, -nb_aged_14to19, -nb_child_ordsecondary, -nb_children_under14, -nb_child_advsecondary,-pssn_childsecondary_value, - nb_children_14andabove, -nb_child_primary, -years_education)
#income outcome: income_raw, nfa_totincome_raw (non-farming income)
# Splitting data into training and testing
set.seed(12345) # For reproducibility
index <- sample(1:nrow(hh_pred), 0.7 * nrow(hh_pred))
train <- hh_pred[index, ]
test <- hh_pred[-index, ]
lm_model = lm(secondary_enrollment_rate~., data=train)
# Get the summary of the model
summary_model <- summary(lm_model)
# Extract coefficients and their p-values
coefficients <- summary_model$coefficients
# Sort coefficients by p-value in ascending order
sorted_coeffs <- coefficients[order(coefficients[, "Pr(>|t|)"]), ]
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_sorted_coeffs <- sorted_coeffs[sorted_coeffs[, "Pr(>|t|)"] < 0.05, ]
# Convert to a data frame for better display
significant_sorted_coeffs_df <- as.data.frame(significant_sorted_coeffs)
# Format p-values to 3 decimal places
significant_sorted_coeffs_df$`Pr(>|t|)` <- format(significant_sorted_coeffs_df$`Pr(>|t|)`, digits = 3, scientific = FALSE)
# Print the results in a nice format
print(significant_sorted_coeffs_df)
predicions = predict(lm_model, newdata = test)
library("caret")       # For training models
# Filter out columns with too many missing values (e.g. >20%)
hh_constructed = read_dta(file.path(Sys.getenv("analysis_baseline_data_dir"), "HHSurvey.dta"))
hh_numeric <- hh_constructed %>% select(where(is.numeric))
threshold = 0.2
hh_pred <- hh_numeric  %>%
select(where(~ mean(is.na(.)) <= threshold))
write.csv(colnames(hh_pred),"hh_pred_cols.csv")
#secondary enrollment rate
hh_pred = hh_pred %>%
filter(nb_children_14andabove > 0) %>%
transform(secondary_enrollment_rate = (nb_child_ordsecondary +nb_child_advsecondary)/ nb_children_14andabove) %>%
filter(!is.na(secondary_enrollment_rate)) %>%
select(-nb_aged_6to13, -nb_aged_14to19, -nb_child_ordsecondary, -nb_children_under14, -nb_child_advsecondary,-pssn_childsecondary_value, - nb_children_14andabove, -nb_child_primary, -years_education)
#income outcome: income_raw, nfa_totincome_raw (non-farming income)
# Splitting data into training and testing
set.seed(12345) # For reproducibility
index <- sample(1:nrow(hh_pred), 0.7 * nrow(hh_pred))
train <- hh_pred[index, ]
test <- hh_pred[-index, ]
library("caret")       # For training models
# Filter out columns with too many missing values (e.g. >20%)
hh_constructed = read_dta(file.path(Sys.getenv("analysis_baseline_data_dir"), "HHSurvey.dta"))
hh_numeric <- hh_constructed %>% select(where(is.numeric))
threshold = 0.2
hh_pred <- hh_numeric  %>%
select(where(~ mean(is.na(.)) <= threshold))
#secondary enrollment rate
hh_pred = hh_pred %>%
filter(nb_children_14andabove > 0) %>%
transform(secondary_enrollment_rate = (nb_child_ordsecondary +nb_child_advsecondary)/ nb_children_14andabove) %>%
filter(!is.na(secondary_enrollment_rate)) %>%
select(-nb_aged_6to13, -nb_aged_14to19, -nb_child_ordsecondary, -nb_children_under14, -nb_child_advsecondary,-pssn_childsecondary_value, - nb_children_14andabove, -nb_child_primary, -years_education)
#income outcome: income_raw, nfa_totincome_raw (non-farming income)
# Splitting data into training and testing
set.seed(12345) # For reproducibility
index <- sample(1:nrow(hh_pred), 0.7 * nrow(hh_pred))
train <- hh_pred[index, ]
test <- hh_pred[-index, ]
lm_model = lm(secondary_enrollment_rate~., data=train)
# Get the summary of the model
summary_model <- summary(lm_model)
# Extract coefficients and their p-values
coefficients <- summary_model$coefficients
# Sort coefficients by p-value in ascending order
sorted_coeffs <- coefficients[order(coefficients[, "Pr(>|t|)"]), ]
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_sorted_coeffs <- sorted_coeffs[sorted_coeffs[, "Pr(>|t|)"] < 0.05, ]
# Convert to a data frame for better display
significant_sorted_coeffs_df <- as.data.frame(significant_sorted_coeffs)
# Format p-values to 3 decimal places
significant_sorted_coeffs_df$`Pr(>|t|)` <- format(significant_sorted_coeffs_df$`Pr(>|t|)`, digits = 3, scientific = FALSE)
# Print the results in a nice format
print(significant_sorted_coeffs_df)
predicions = predict(lm_model, newdata = test)
tr = test
tr[is.na(tr)] <- mean(tr, na.rm = TRUE)
tr[is.nan(tr)] <- mean(tr, na.rm = TRUE)
tr[is.na(tr)] <- mean(test, na.rm = TRUE)
tr[is.nan(tr)] <- mean(test, na.rm = TRUE)
library("caret")       # For training models
# Filter out columns with too many missing values (e.g. >20%)
hh_constructed = read_dta(file.path(Sys.getenv("analysis_baseline_data_dir"), "HHSurvey.dta"))
hh_numeric <- hh_constructed %>% select(where(is.numeric))
threshold = 0.2
hh_pred <- hh_numeric  %>%
select(where(~ mean(is.na(.)) <= threshold))
#secondary enrollment rate
hh_pred = hh_pred %>%
filter(nb_children_14andabove > 0) %>%
transform(secondary_enrollment_rate = (nb_child_ordsecondary +nb_child_advsecondary)/ nb_children_14andabove) %>%
filter(!is.na(secondary_enrollment_rate)) %>%
select(-nb_aged_6to13, -nb_aged_14to19, -nb_child_ordsecondary, -nb_children_under14, -nb_child_advsecondary,-pssn_childsecondary_value, - nb_children_14andabove, -nb_child_primary, -years_education)
#income outcome: income_raw, nfa_totincome_raw (non-farming income)
# Splitting data into training and testing
set.seed(12345) # For reproducibility
index <- sample(1:nrow(hh_pred), 0.7 * nrow(hh_pred))
train <- hh_pred[index, ] %>% na.omit(train)
test <- hh_pred[-index, ] %>% na.omit(test)
lm_model = lm(secondary_enrollment_rate~., data=train)
# Get the summary of the model
summary_model <- summary(lm_model)
# Extract coefficients and their p-values
coefficients <- summary_model$coefficients
# Sort coefficients by p-value in ascending order
sorted_coeffs <- coefficients[order(coefficients[, "Pr(>|t|)"]), ]
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_sorted_coeffs <- sorted_coeffs[sorted_coeffs[, "Pr(>|t|)"] < 0.05, ]
# Convert to a data frame for better display
significant_sorted_coeffs_df <- as.data.frame(significant_sorted_coeffs)
# Format p-values to 3 decimal places
significant_sorted_coeffs_df$`Pr(>|t|)` <- format(significant_sorted_coeffs_df$`Pr(>|t|)`, digits = 3, scientific = FALSE)
# Print the results in a nice format
print(significant_sorted_coeffs_df)
predicions = predict(lm_model, newdata = test)
predicions = predict(lm_model, test)
predicions = predict(lm_model, train)
anyNA(test)
any(is.nan(test))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
setwd("C:/Users/wb629244/PSSN/pssn2livelihood/03_Endline/03_Endline_HFCs/02_HFC_check_Gdrive_R_prototype")
# Load necessary libraries
library(tidyverse)
library(haven)
library(lubridate)
library(dplyr)
library(tidyr)
library(ggplot2)
source("global_env_setup.R")
source("HFC_utils.R")
library("caret")       # For training models
# Filter out columns with too many missing values (e.g. >20%)
hh_constructed = read_dta(file.path(Sys.getenv("analysis_baseline_data_dir"), "HHSurvey.dta"))
hh_numeric <- hh_constructed %>% select(where(is.numeric))
threshold = 0.2
hh_pred <- hh_numeric  %>%
select(where(~ mean(is.na(.)) <= threshold))
#secondary enrollment rate
hh_pred = hh_pred %>%
filter(nb_children_14andabove > 0) %>%
transform(secondary_enrollment_rate = (nb_child_ordsecondary +nb_child_advsecondary)/ nb_children_14andabove) %>%
filter(!is.na(secondary_enrollment_rate)) %>%
select(-nb_aged_6to13, -nb_aged_14to19, -nb_child_ordsecondary, -nb_children_under14, -nb_child_advsecondary,-pssn_childsecondary_value, - nb_children_14andabove, -nb_child_primary, -years_education)
#income outcome: income_raw, nfa_totincome_raw (non-farming income)
# Splitting data into training and testing
set.seed(123) # For reproducibility
index <- sample(1:nrow(hh_pred), 0.7 * nrow(hh_pred))
train <- hh_pred[index, ] %>% na.omit(train)
test <- hh_pred[-index, ] %>% na.omit(test)
lm_model = lm(secondary_enrollment_rate~., data=train)
# Get the summary of the model
summary_model <- summary(lm_model)
# Extract coefficients and their p-values
coefficients <- summary_model$coefficients
# Sort coefficients by p-value in ascending order
sorted_coeffs <- coefficients[order(coefficients[, "Pr(>|t|)"]), ]
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_sorted_coeffs <- sorted_coeffs[sorted_coeffs[, "Pr(>|t|)"] < 0.05, ]
# Convert to a data frame for better display
significant_sorted_coeffs_df <- as.data.frame(significant_sorted_coeffs)
# Format p-values to 3 decimal places
significant_sorted_coeffs_df$`Pr(>|t|)` <- format(significant_sorted_coeffs_df$`Pr(>|t|)`, digits = 3, scientific = FALSE)
# Print the results in a nice format
print(significant_sorted_coeffs_df)
predicions = predict(lm_model, test)
mse_lm <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
lm_model = lm(secondary_enrollment_rate~., data=train)
library("caret")       # For training models
# Filter out columns with too many missing values (e.g. >20%)
hh_constructed = read_dta(file.path(Sys.getenv("analysis_baseline_data_dir"), "HHSurvey.dta"))
hh_numeric <- hh_constructed %>% select(where(is.numeric))
threshold = 0.2
hh_pred <- hh_numeric  %>%
select(where(~ mean(is.na(.)) <= threshold))
#secondary enrollment rate
hh_pred = hh_pred %>%
filter(nb_children_14andabove > 0) %>%
transform(secondary_enrollment_rate = (nb_child_ordsecondary +nb_child_advsecondary)/ nb_children_14andabove) %>%
filter(!is.na(secondary_enrollment_rate)) %>%
select(-nb_aged_6to13, -nb_aged_14to19, -nb_child_ordsecondary, -nb_children_under14, -nb_child_advsecondary,-pssn_childsecondary_value, - nb_children_14andabove, -nb_child_primary, -years_education)
#income outcome: income_raw, nfa_totincome_raw (non-farming income)
# Splitting data into training and testing
set.seed(123) # For reproducibility
index <- sample(1:nrow(hh_pred), 0.7 * nrow(hh_pred))
train <- hh_pred[index, ] %>% na.omit(train)
test <- hh_pred[-index, ] %>% na.omit(test)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
setwd("C:/Users/wb629244/PSSN/pssn2livelihood/03_Endline/03_Endline_HFCs/02_HFC_check_Gdrive_R_prototype")
# Load necessary libraries
library(tidyverse)
library(haven)
library(lubridate)
library(dplyr)
library(tidyr)
library(ggplot2)
source("global_env_setup.R")
source("HFC_utils.R")
lm_model = lm(secondary_enrollment_rate~., data=train)
# Extract coefficients and summary
model_summary <- summary(lm_model)
coefficients <- model_summary$coefficients
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_coeffs <- coefficients[coefficients[, "Pr(>|t|)"] < 0.05, ]
# Sort coefficients by their absolute value
significant_coeffs <- cbind(significant_coeffs, Abs_Estimate = abs(significant_coeffs[, "Estimate"]))
significant_coeffs_sorted <- significant_coeffs[order(significant_coeffs[, "Abs_Estimate"], decreasing = TRUE), ]
significant_coeffs_sorted <- significant_coeffs_sorted[, -ncol(significant_coeffs_sorted)]
# Convert to a data frame and format
significant_coeffs_sorted_df <- as.data.frame(significant_coeffs_sorted)
significant_coeffs_sorted_df <- significant_coeffs_sorted_df %>%
mutate(across(where(is.numeric), ~ formatC(., format = "f", digits = 3)))
# Print the table using knitr::kable() or kableExtra
kable(significant_coeffs_sorted_df, caption = "Sorted Significant Linear Regression Coefficients") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
predicions = predict(lm_model, test)
mse_lm <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
lm_model = lm(secondary_enrollment_rate~., data=train)
# Extract coefficients and summary
model_summary <- summary(lm_model)
coefficients <- model_summary$coefficients
# Filter for significant coefficients (e.g., p-value < 0.05)
significant_coeffs <- coefficients[coefficients[, "Pr(>|t|)"] <= 0.01, ]
# Sort coefficients by their absolute value
significant_coeffs <- cbind(significant_coeffs, Abs_Estimate = abs(significant_coeffs[, "Estimate"]))
significant_coeffs_sorted <- significant_coeffs[order(significant_coeffs[, "Abs_Estimate"], decreasing = TRUE), ]
significant_coeffs_sorted <- significant_coeffs_sorted[, -ncol(significant_coeffs_sorted)]
# Convert to a data frame and format
significant_coeffs_sorted_df <- as.data.frame(significant_coeffs_sorted)
significant_coeffs_sorted_df <- significant_coeffs_sorted_df %>%
mutate(across(where(is.numeric), ~ formatC(., format = "f", digits = 3)))
# Print the table using knitr::kable() or kableExtra
kable(significant_coeffs_sorted_df, caption = "Sorted Significant Linear Regression Coefficients") %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
predicions = predict(lm_model, test)
mse_lm <- mean((predicions - test$secondary_enrollment_rate)^2, na.rm = TRUE)
library("knitr")
library("kableExtra")
drive_auth()
library(googledrive)
#The following line authenticate the Google account access via a browser, only needed for the first time
drive_auth()
library(googledrive)
#The following line authenticate the Google account access via a browser, only needed for the first time
drive_auth()
library(googledrive)
#The following line authenticate the Google account access via a browser, only needed for the first time
drive_auth()
drive_deauth()
drive_auth()
